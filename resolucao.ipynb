{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c954d71",
   "metadata": {},
   "source": [
    "### Importar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6df2f64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "credit = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "display(credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142f94e",
   "metadata": {},
   "source": [
    "### Analisar necessidade de tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e4a82a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "inexistência de linhas vazias e Datatypes corretos\n",
    "'''\n",
    "\n",
    "print(credit.info())\n",
    "\n",
    "print(credit[\"Class\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b09759",
   "metadata": {},
   "source": [
    "### Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44a53a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.522992</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.391253</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.394557</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.312697</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.480237</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.446013</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402727</td>\n",
       "      <td>0.415489</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>0.662607</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.389197</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.314371</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.938617</td>\n",
       "      <td>0.776520</td>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.269796</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.782484</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561327</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.401270</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>0.999965</td>\n",
       "      <td>0.756448</td>\n",
       "      <td>0.873531</td>\n",
       "      <td>0.666991</td>\n",
       "      <td>0.160317</td>\n",
       "      <td>0.729603</td>\n",
       "      <td>0.236810</td>\n",
       "      <td>0.235393</td>\n",
       "      <td>0.863749</td>\n",
       "      <td>0.528729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564920</td>\n",
       "      <td>0.515249</td>\n",
       "      <td>0.680500</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.658558</td>\n",
       "      <td>0.466291</td>\n",
       "      <td>0.433929</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.945845</td>\n",
       "      <td>0.766677</td>\n",
       "      <td>0.872678</td>\n",
       "      <td>0.219189</td>\n",
       "      <td>0.771561</td>\n",
       "      <td>0.273661</td>\n",
       "      <td>0.265504</td>\n",
       "      <td>0.788548</td>\n",
       "      <td>0.482925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564933</td>\n",
       "      <td>0.553153</td>\n",
       "      <td>0.665619</td>\n",
       "      <td>0.245298</td>\n",
       "      <td>0.543855</td>\n",
       "      <td>0.360884</td>\n",
       "      <td>0.417775</td>\n",
       "      <td>0.312038</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.990905</td>\n",
       "      <td>0.764080</td>\n",
       "      <td>0.781102</td>\n",
       "      <td>0.227202</td>\n",
       "      <td>0.783425</td>\n",
       "      <td>0.293496</td>\n",
       "      <td>0.263547</td>\n",
       "      <td>0.792985</td>\n",
       "      <td>0.477677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565220</td>\n",
       "      <td>0.537005</td>\n",
       "      <td>0.664877</td>\n",
       "      <td>0.468492</td>\n",
       "      <td>0.592824</td>\n",
       "      <td>0.411177</td>\n",
       "      <td>0.416593</td>\n",
       "      <td>0.312585</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.954209</td>\n",
       "      <td>0.772856</td>\n",
       "      <td>0.849587</td>\n",
       "      <td>0.282508</td>\n",
       "      <td>0.763172</td>\n",
       "      <td>0.269291</td>\n",
       "      <td>0.261175</td>\n",
       "      <td>0.792671</td>\n",
       "      <td>0.476287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565755</td>\n",
       "      <td>0.547353</td>\n",
       "      <td>0.663008</td>\n",
       "      <td>0.398836</td>\n",
       "      <td>0.545958</td>\n",
       "      <td>0.514746</td>\n",
       "      <td>0.418520</td>\n",
       "      <td>0.315245</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949232</td>\n",
       "      <td>0.765256</td>\n",
       "      <td>0.849601</td>\n",
       "      <td>0.229488</td>\n",
       "      <td>0.765632</td>\n",
       "      <td>0.256488</td>\n",
       "      <td>0.274963</td>\n",
       "      <td>0.780938</td>\n",
       "      <td>0.479528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565688</td>\n",
       "      <td>0.540031</td>\n",
       "      <td>0.671029</td>\n",
       "      <td>0.383420</td>\n",
       "      <td>0.551319</td>\n",
       "      <td>0.291786</td>\n",
       "      <td>0.416466</td>\n",
       "      <td>0.313401</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0       0.000000  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669   \n",
       "1       0.000000  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192   \n",
       "2       0.000006  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122   \n",
       "3       0.000006  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559   \n",
       "4       0.000012  0.938617  0.776520  0.864251  0.269796  0.762975  0.263984   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.999965  0.756448  0.873531  0.666991  0.160317  0.729603  0.236810   \n",
       "284803  0.999971  0.945845  0.766677  0.872678  0.219189  0.771561  0.273661   \n",
       "284804  0.999977  0.990905  0.764080  0.781102  0.227202  0.783425  0.293496   \n",
       "284805  0.999977  0.954209  0.772856  0.849587  0.282508  0.763172  0.269291   \n",
       "284806  1.000000  0.949232  0.765256  0.849601  0.229488  0.765632  0.256488   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.266815  0.786444  0.475312  ...  0.561184  0.522992  0.663793   \n",
       "1       0.264875  0.786298  0.453981  ...  0.557840  0.480237  0.666938   \n",
       "2       0.270177  0.788042  0.410603  ...  0.565477  0.546030  0.678939   \n",
       "3       0.266803  0.789434  0.414999  ...  0.559734  0.510277  0.662607   \n",
       "4       0.268968  0.782484  0.490950  ...  0.561327  0.547271  0.663392   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284802  0.235393  0.863749  0.528729  ...  0.564920  0.515249  0.680500   \n",
       "284803  0.265504  0.788548  0.482925  ...  0.564933  0.553153  0.665619   \n",
       "284804  0.263547  0.792985  0.477677  ...  0.565220  0.537005  0.664877   \n",
       "284805  0.261175  0.792671  0.476287  ...  0.565755  0.547353  0.663008   \n",
       "284806  0.274963  0.780938  0.479528  ...  0.565688  0.540031  0.671029   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  Class  \n",
       "0       0.391253  0.585122  0.394557  0.418976  0.312697  0.005824      0  \n",
       "1       0.336440  0.587290  0.446013  0.416345  0.313423  0.000105      0  \n",
       "2       0.289354  0.559515  0.402727  0.415489  0.311911  0.014739      0  \n",
       "3       0.223826  0.614245  0.389197  0.417669  0.314371  0.004807      0  \n",
       "4       0.401270  0.566343  0.507497  0.420561  0.317490  0.002724      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "284802  0.313600  0.658558  0.466291  0.433929  0.329840  0.000030      0  \n",
       "284803  0.245298  0.543855  0.360884  0.417775  0.312038  0.000965      0  \n",
       "284804  0.468492  0.592824  0.411177  0.416593  0.312585  0.002642      0  \n",
       "284805  0.398836  0.545958  0.514746  0.418520  0.315245  0.000389      0  \n",
       "284806  0.383420  0.551319  0.291786  0.416466  0.313401  0.008446      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credit_norm = credit.drop(\"Class\", axis=1)\n",
    "\n",
    "y = credit[\"Class\"]             \n",
    "\n",
    "credit_norm = credit_norm.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "credit_norm = pd.concat([credit_norm, y], axis=1)\n",
    "\n",
    "display(credit_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8a5ea",
   "metadata": {},
   "source": [
    "### Análise PCA para redução da dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23d5f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Coluna      Peso\n",
      "0     Time  0.611059\n",
      "1       V1  0.053329\n",
      "2       V2  0.049627\n",
      "3       V3  0.047362\n",
      "4       V4  0.036554\n",
      "5       V5  0.032352\n",
      "6       V6  0.031363\n",
      "7       V7  0.027712\n",
      "8       V8  0.026001\n",
      "9       V9  0.011472\n",
      "10     V10  0.011216\n",
      "11     V11  0.009078\n",
      "12     V12  0.008798\n",
      "13     V13  0.008247\n",
      "14     V14  0.006617\n",
      "15     V15  0.006219\n",
      "16     V16  0.004907\n",
      "17     V17  0.004138\n",
      "18     V18  0.003899\n",
      "19     V19  0.002694\n",
      "20     V20  0.001468\n",
      "21     V21  0.001319\n",
      "22     V22  0.001134\n",
      "23     V23  0.000845\n",
      "24     V24  0.000689\n",
      "25     V25  0.000588\n",
      "26     V26  0.000474\n",
      "27     V27  0.000444\n",
      "28     V28  0.000359\n",
      "29  Amount  0.000035\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pca_df = credit_norm.drop(\"Class\", axis=1)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(pca_df)\n",
    "\n",
    "impacto_pca = pca.explained_variance_ratio_\n",
    "\n",
    "colunas = list(pca_df.columns)\n",
    "dic = dict()\n",
    "for i in range(len(impacto_pca)):\n",
    "    dic[colunas[i]] = impacto_pca[i]\n",
    "\n",
    "df = pd.DataFrame({\"Coluna\": colunas, \"Peso\": impacto_pca})\n",
    "\n",
    "print(df.sort_values(by=\"Peso\", ascending=False))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.barplot(x=df[\"Coluna\"], y=df[\"Peso\"])\n",
    "ax.tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb2bbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Considerando a variabilidade dos dados em menos de 1%, as colunas de V11 a V28 serão eliminadas. Amount tbm \n",
    "'''\n",
    "\n",
    "credit_norm = credit_norm.drop([\"V11\", \"V12\", \"V13\", \"V14\",\"V15\", \"V16\",\"V17\", \"V18\",\"V19\", \"V20\",\"V21\", \"V22\",\"V23\", \"V24\",\"V25\", \"V26\",\"V27\", \"V28\",\"Amount\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab2655",
   "metadata": {},
   "source": [
    "### Identificar necessidade de balanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d807765b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Dataset encontra-se com uma grande discrepância em relação à qtd de amostras para 0 e para 1\n",
    "'''\n",
    "\n",
    "credit_norm[[\"Class\"]].groupby(\"Class\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a99c8",
   "metadata": {},
   "source": [
    "### Aplicação do Método SMOTE para balanceamento da qtd de amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18f1f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "sm = credit_norm.copy()\n",
    "\n",
    "# dividir df em X e y\n",
    "X = sm.drop(\"Class\", axis=1)\n",
    "y = sm[\"Class\"]\n",
    "\n",
    "# aplicar método\n",
    "smote = SMOTE(sampling_strategy=\"minority\", k_neighbors=23)\n",
    "X_sm, y_sm = smote.fit_resample(X, y)\n",
    "\n",
    "credit_adj = pd.concat([X_sm, y_sm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b75ae0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0       0.000000  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669   \n",
      "1       0.000000  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192   \n",
      "2       0.000006  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122   \n",
      "3       0.000006  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559   \n",
      "4       0.000012  0.938617  0.776520  0.864251  0.269796  0.762975  0.263984   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "568625  0.209033  0.870698  0.824793  0.724282  0.510148  0.736042  0.245995   \n",
      "568626  0.586663  0.861638  0.825283  0.693088  0.611084  0.742073  0.243636   \n",
      "568627  0.393633  0.976969  0.761815  0.856484  0.293165  0.756631  0.260411   \n",
      "568628  0.238490  0.808265  0.831696  0.634991  0.565590  0.711027  0.225887   \n",
      "568629  0.897033  0.985304  0.780382  0.801683  0.431137  0.775955  0.253320   \n",
      "\n",
      "              V7        V8        V9       V10  \n",
      "0       0.266815  0.786444  0.475312  0.510600  \n",
      "1       0.264875  0.786298  0.453981  0.505267  \n",
      "2       0.270177  0.788042  0.410603  0.513018  \n",
      "3       0.266803  0.789434  0.414999  0.507585  \n",
      "4       0.268968  0.782484  0.490950  0.524303  \n",
      "...          ...       ...       ...       ...  \n",
      "568625  0.224452  0.823254  0.275273  0.358082  \n",
      "568626  0.221498  0.820666  0.308800  0.339819  \n",
      "568627  0.261861  0.785314  0.458509  0.519025  \n",
      "568628  0.188463  0.847265  0.241157  0.233584  \n",
      "568629  0.268483  0.784086  0.411154  0.508361  \n",
      "\n",
      "[568630 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ce6af",
   "metadata": {},
   "source": [
    "### Replicação da ideia por trás do algoritmo de perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "023c4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definir funções auxiliares\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# definição aleatória de pesos\n",
    "def weight_random(length):\n",
    "    return [random.random() for _ in range(length)]\n",
    "\n",
    "\n",
    "# soma da multiplicação entre as variáveis e seus respectivos pesos\n",
    "def weighted_sum(x_input, w_weight):\n",
    "    soma = 0\n",
    "    for x, w in zip(x_input, w_weight):\n",
    "        soma += x*w\n",
    "        \n",
    "    return soma\n",
    "\n",
    "\n",
    "# implementação da ideia de não linearidade a partir de uma curva logística\n",
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))\n",
    "\n",
    "\n",
    "# definição da previsão a partir do threshold\n",
    "def predict(prob, threshold):\n",
    "    if prob >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# função final\n",
    "def perceptron(x_input, w_weight, threshold):\n",
    "    \n",
    "    # soma com pesos de todas variaveis\n",
    "    soma = weighted_sum(x_input, w_weight)\n",
    "    \n",
    "    # aplicação de não linearidade com regressão logística\n",
    "    prob = sigmoid(soma)\n",
    "    \n",
    "    return prob, predict(prob, threshold)\n",
    "\n",
    "\n",
    "# função de cálculo dos custos associados ao erro pelo princípio de cross entropy loss\n",
    "def cross_entropy_loss(w_sum, target, prediction):\n",
    "    \n",
    "    # criar dataframe\n",
    "    data = {\"w_sum\": w_sum, \"target\": target, \"predicted\": prediction}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # cálculo do custo por linha\n",
    "    df[\"loss\"] = - (df[\"target\"]*np.log10(df[\"w_sum\"]) + (1-df[\"target\"])*np.log10(1-df[\"w_sum\"]))\n",
    "    \n",
    "    # retornar custo total do modelo\n",
    "    return df[\"loss\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1110935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média Predictions: 0.9478248672458001\n",
      "Acurácia: 0.5001359995497946\n",
      "Avarage Loss: 0.66443116649119\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Implementação inicial com pesos aleatórios\n",
    "'''\n",
    "\n",
    "# divisão de 75% dos dados para treino e 25% para teste\n",
    "treino = credit_adj.sample(frac=0.75).reset_index()\n",
    "treino = treino.drop(\"index\", axis=1)\n",
    "teste = credit_adj[~credit_adj.apply(tuple,1).isin(treino.apply(tuple,1))].reset_index() # cruzar treino e credit_adj e pegar apenas linhas que não aparecem em ambas\n",
    "teste = teste.drop(\"index\", axis=1)\n",
    "\n",
    "# dividir df em features e label\n",
    "entrada = treino.drop(\"Class\", axis=1)\n",
    "target = list(treino[\"Class\"])\n",
    "\n",
    "# dicionario resumo\n",
    "interacoes = {}\n",
    "cont = 0\n",
    "\n",
    "interacao = {}\n",
    "\n",
    "# definir pesos aleatoriamente\n",
    "pesos = weight_random(len(list(entrada.columns)))\n",
    "\n",
    "# lista de predição\n",
    "prediction = []\n",
    "w_sum = []\n",
    "\n",
    "    \n",
    "# iterar sobre cada amostra treino\n",
    "for linha in entrada.index:\n",
    "\n",
    "    # variaveis de entrada\n",
    "    x_input = list(entrada.iloc[linha, :])\n",
    "        \n",
    "    # na primeira iteração, os pesos são aleatórios\n",
    "    soma, p = perceptron(x_input, pesos, 0.5)\n",
    "\n",
    "    # adicionar previsao na lista predict\n",
    "    prediction.append(p)\n",
    "\n",
    "    # adicionar soma para cálculo do custo total\n",
    "    w_sum.append(soma)\n",
    "    \n",
    "# cálculo do custo\n",
    "loss = cross_entropy_loss(w_sum, target, prediction)\n",
    "\n",
    "# cálculo da acurácia\n",
    "certos = 0\n",
    "for y, pred in zip(target, prediction):\n",
    "    if y == pred:\n",
    "        certos = certos + 1\n",
    "        \n",
    "acuracia = certos / len(target)\n",
    "\n",
    "interacao[\"weights\"] = pesos\n",
    "interacao[\"w_sum\"] = w_sum\n",
    "interacao[\"prediction\"] = prediction\n",
    "interacao[\"loss\"] = loss\n",
    "interacao[\"accuracy\"] = acuracia\n",
    "\n",
    "interacoes[0] = interacao.copy()\n",
    "\n",
    "media = sum(w_sum)/len(w_sum)\n",
    "\n",
    "print(f\"Média Predictions: {media}\")\n",
    "print(f\"Acurácia: {acuracia}\")\n",
    "print(f\"Avarage Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e280d",
   "metadata": {},
   "source": [
    "### Aplicação da ideia de Gradient Descent e Mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2d575de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5297616557242247\n",
      "0.9289648900291193\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Considerando a ideia de um processo estocástico (\"andar de um bêbado\"), a descida do gradiente tende a não ser linear, logo\n",
    "esse primeiro formato testado pode não ser o ideal, uma vez que se restringe a poucas iterações e assume a linearidade\n",
    "do processo. Na célula abaixo, tentarei aplicar valores fixos para epoch, buscando perceber se a acurácia é otimizada\n",
    "'''\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_0 = 10**10\n",
    "loss_1 = loss\n",
    "\n",
    "new_pesos = {}\n",
    "\n",
    "\n",
    "# loop até encontrar o mínimo local\n",
    "while loss_0 > loss_1:\n",
    "    \n",
    "    interacao.clear()\n",
    "    \n",
    "    cont += 1\n",
    "    \n",
    "    w_sum.clear()\n",
    "    \n",
    "    # batch de 500 amostras\n",
    "    batch = entrada.sample(500)\n",
    "    \n",
    "    \n",
    "    # definir novos pesos para cada amostra\n",
    "    for b in batch.index:\n",
    "        n_pesos = [w - (learning_rate*(target[b]-prediction[b])) for w in pesos]\n",
    "        \n",
    "        # retomar valor original caso determinado peso tenha sido zerado pela regularização na iteração interior\n",
    "        current = cont\n",
    "        while 0 in n_pesos:\n",
    "            index = n_pesos.index(0)\n",
    "            n_pesos[index] = interacoes[current-1][\"weights\"][index]\n",
    "            current -= 1\n",
    "        \n",
    "        new_pesos[b] = n_pesos\n",
    "    \n",
    "    # pegar a média dos pesos para cada amostra\n",
    "    somas = [0] * len(pesos)\n",
    "    \n",
    "    for i in range(len(pesos)):  # somar valores\n",
    "        for k in new_pesos.keys():\n",
    "            somas[i] += new_pesos[k][i]\n",
    "        \n",
    "    pesos = [som/len(batch) for som in somas]  # encontrar médias\n",
    "    \n",
    "    # regularization (dropout de 10%)\n",
    "    for j in range(math.floor(0.1*len(pesos))):\n",
    "        n1 = random.randint(0,len(pesos)-1)\n",
    "        #n2 = n1\n",
    "        #while n2 == n1:\n",
    "            #n2 = random.randint(0,len(pesos)-1)\n",
    "            \n",
    "    pesos[n1] = 0\n",
    "    #pesos[n2] = 0\n",
    "            \n",
    "    prediction.clear()\n",
    "    \n",
    "    # iterar sobre cada linha\n",
    "    for linha in entrada.index:\n",
    "        \n",
    "        # variaveis de entrada\n",
    "        x_input = list(entrada.iloc[linha, :])\n",
    "\n",
    "        # na primeira iteração, os pesos são aleatórios\n",
    "        prob, p = perceptron(x_input, pesos, 0.5)\n",
    "\n",
    "        # adicionar previsao na lista predict\n",
    "        prediction.append(p)\n",
    "\n",
    "        # adicionar soma para cálculo do custo\n",
    "        w_sum.append(prob)\n",
    "\n",
    "    # cálculo do custo\n",
    "    loss = cross_entropy_loss(w_sum, target, prediction)\n",
    "    \n",
    "    # cálculo da acurácia\n",
    "    certos = 0\n",
    "    for y, pred in zip(target, prediction):\n",
    "        if y == pred:\n",
    "            certos = certos + 1\n",
    "        \n",
    "    acuracia = certos / len(target)\n",
    "    \n",
    "    interacao[\"weights\"] = pesos\n",
    "    interacao[\"w_sum\"] = w_sum\n",
    "    interacao[\"prediction\"] = prediction\n",
    "    interacao[\"loss\"] = loss\n",
    "    interacao[\"accuracy\"] = acuracia\n",
    "    \n",
    "    interacoes[cont] = interacao.copy()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "    # atualizar custo\n",
    "    loss_0 = loss_1\n",
    "    \n",
    "    loss_1 = loss\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "daa1ba17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\n",
      "Loss: 0.5538731518610125\n",
      "Accuracy: 0.5001359995497946\n",
      "\n",
      "Epoch: 2/50\n",
      "Loss: 1.1415940576280115\n",
      "Accuracy: 0.5001359995497946\n",
      "\n",
      "Epoch: 3/50\n",
      "Loss: 2.2456137102493976\n",
      "Accuracy: 0.5001359995497946\n",
      "\n",
      "Epoch: 4/50\n",
      "Loss: 3.6177910400238678\n",
      "Accuracy: 0.5001359995497946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/50\n",
      "Loss: inf\n",
      "Accuracy: 0.5001359995497946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/50\n",
      "Loss: inf\n",
      "Accuracy: 0.5001359995497946\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# iterar sobre cada linha\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m linha \u001b[38;5;129;01min\u001b[39;00m entrada\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m     63\u001b[0m     \n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# variaveis de entrada\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     x_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mentrada\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlinha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# na primeira iteração, os pesos são aleatórios\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     prob, p \u001b[38;5;241m=\u001b[39m perceptron(x_input, pesos, \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py:759\u001b[0m, in \u001b[0;36mIndexOpsMixin.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values)\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Decréscimo do gradiente a partir de uma epoch fixada previamente (inicialmente com valor = 50)\n",
    "Mudanças adicionais foram realizadas, como a substituição do threshhold igual à média (agora = 0.5)\n",
    "'''\n",
    "import math\n",
    "\n",
    "\n",
    "# definição das variáveis\n",
    "learning_rate = 0.001\n",
    "\n",
    "EPOCH = 50 \n",
    "\n",
    "# lista que será atualizada com os novos pesos\n",
    "new_pesos = {}\n",
    "\n",
    "\n",
    "# loop até encontrar o mínimo local\n",
    "for inter in range(EPOCH):\n",
    "    \n",
    "    cont += 1   \n",
    "    \n",
    "    # batch de 500 amostras\n",
    "    batch = entrada.sample(500)\n",
    "    \n",
    "    \n",
    "    # definir novos pesos para cada amostra\n",
    "    for b in batch.index:\n",
    "        n_pesos = [w - (learning_rate*(target[b]-prediction[b])) for w in pesos]\n",
    "        \n",
    "        # retomar valor original caso determinado peso tenha sido zerado pela regularização na iteração interior\n",
    "        current = inter\n",
    "        while 0 in n_pesos:\n",
    "            index = n_pesos.index(0)\n",
    "            n_pesos[index] = interacoes[current-1][\"weights\"][index]\n",
    "            current -= 1\n",
    "        \n",
    "        new_pesos[b] = n_pesos\n",
    "    \n",
    "    # pegar a média dos pesos para cada amostra\n",
    "    somas = [0] * len(pesos)\n",
    "    \n",
    "    for i in range(len(pesos)):  # somar valores\n",
    "        for k in new_pesos.keys():\n",
    "            somas[i] += new_pesos[k][i]\n",
    "        \n",
    "    pesos = [som/len(batch) for som in somas]  # encontrar médias\n",
    "    \n",
    "    # regularization (dropout de 10%)\n",
    "    for j in range(math.floor(0.1*len(pesos))):\n",
    "        n1 = random.randint(0,len(pesos)-1)\n",
    "        #n2 = n1\n",
    "        #while n2 == n1:\n",
    "        #    n2 = random.randint(0,len(pesos)-1)\n",
    "            \n",
    "    pesos[n1] = 0\n",
    "    #pesos[n2] = 0\n",
    "            \n",
    "    prediction.clear()\n",
    "    w_sum.clear()\n",
    "    \n",
    "    # iterar sobre cada linha\n",
    "    for linha in entrada.index:\n",
    "        \n",
    "        # variaveis de entrada\n",
    "        x_input = list(entrada.iloc[linha, :])\n",
    "\n",
    "        # na primeira iteração, os pesos são aleatórios\n",
    "        prob, p = perceptron(x_input, pesos, 0.5)\n",
    "\n",
    "        # adicionar previsao na lista predict\n",
    "        prediction.append(p)\n",
    "\n",
    "        # adicionar prob para cálculo do custo final\n",
    "        w_sum.append(prob)\n",
    "\n",
    "    # cálculo do custo\n",
    "    loss = cross_entropy_loss(w_sum, target, prediction)\n",
    "    \n",
    "    # cálculo da acurácia\n",
    "    certos = 0\n",
    "    for y, pred in zip(target, prediction):\n",
    "        if y == pred:\n",
    "            certos = certos + 1\n",
    "        \n",
    "    acuracia = certos / len(target)\n",
    "    \n",
    "    interacao[\"weights\"] = pesos\n",
    "    interacao[\"w_sum\"] = w_sum\n",
    "    interacao[\"prediction\"] = prediction\n",
    "    interacao[\"loss\"] = loss\n",
    "    interacao[\"accuracy\"] = acuracia\n",
    "    \n",
    "    interacoes[cont] = interacao.copy()\n",
    "    \n",
    "    interacao.clear()\n",
    "    \n",
    "    print(f\"Epoch: {inter+1}/{EPOCH}\")\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Accuracy: {acuracia}\")\n",
    "    print()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6572d",
   "metadata": {},
   "source": [
    "### Evolução da acurácia e do custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e586b912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracies = [interacoes[i][\"accuracy\"] for i in interacoes.keys()]\n",
    "losses = [interacoes[i][\"loss\"] for i in interacoes.keys()]\n",
    "\n",
    "# Criar eixo x para as iterações\n",
    "iterations_axis = range(1, len(interacoes) + 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(iterations_axis, accuracies, label=\"Acurácia\")\n",
    "plt.plot(iterations_axis, losses, label=\"Custos\")\n",
    "plt.xlabel(\"Iterações\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4648ea6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000712725439920274, 0.0002821218052766025, 0, 0.0006992277326778647, 0, 0.0003882612703631155, 0.0005800591496488023, 0.0007328502279859439, 0.0008665648908360141, 0.0008790406120591235, 0.0008278807750205125]\n",
      "0.5006026186947795\n"
     ]
    }
   ],
   "source": [
    "print(interacoes[50][\"weights\"])\n",
    "print(interacoes[50][\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c600976a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8901299010261012, 0.8541284532587126, 0.12587814366099037, 0.23872906157980667, 0.6318982266653775, 0.681465381062337, 0.2906754319587034, 0.032329971135661495, 0.3369827647622594, 0.7410232697810613, 0.7818137157510711]\n",
      "\n",
      "[0.8906559010261018, 0.8546544532587178, 0, 0.23925506157980644, 0.6324242266653793, 0.6819913810623367, 0.29120143195870485, 0.032855971135661376, 0.3375087647622613, 0.7415492697810623, 0.7823397157510674]\n",
      "\n",
      "[1.7782692824480832, 0, 0.12641863108634513, 0.4780732069132874, 1.2628388604241083, 1.3617749006004245, 0.58175816218958, 0.06610062238677965, 0.6741875984654687, 1.4806524464830184, 1.5620701766391105]\n",
      "\n",
      "[3.557064564896114, 1.706936392704376, 0.2533632621726923, 0.9566724138265837, 2.5262037208482493, 2.7240758012008772, 0, 0.13272724477355927, 1.3489011969309228, 2.961830892965991, 3.1246663532781684]\n",
      "\n",
      "[7.114649129792301, 3.414392785408756, 0.5072465243453786, 1.913864827653167, 5.05292744169654, 5.448671602401725, 1.1645623243791579, 0.26597448954711633, 0, 5.924181785932007, 6.249852706556373]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(5):\n",
    "    print(interacoes[x][\"weights\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "197ee622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(5):\n",
    "    print(interacoes[x][\"w_sum\"][:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a37f5",
   "metadata": {},
   "source": [
    "### Validação dos pesos nas variaveis teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa4606f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média Predictions: 0.500581585997753\n",
      "Acurácia: 0.5011105452281923\n",
      "Avarage Loss: 0.3010497115094095\n"
     ]
    }
   ],
   "source": [
    "# dividir df em features e label\n",
    "entrada1 = teste.drop(\"Class\", axis=1)\n",
    "target1 = list(teste[\"Class\"])\n",
    "\n",
    "# definir pesos\n",
    "pesos = interacoes[2][\"weights\"]\n",
    "\n",
    "# lista de predição\n",
    "prediction = []\n",
    "w_sum = []\n",
    "\n",
    "    \n",
    "# iterar sobre cada amostra\n",
    "for linha in entrada1.index:\n",
    "\n",
    "    # variaveis de entrada\n",
    "    x_input = list(entrada1.iloc[linha, :])\n",
    "        \n",
    "    # na primeira iteração, os pesos são aleatórios\n",
    "    soma, p = perceptron(x_input, pesos, 0.5)\n",
    "\n",
    "    # adicionar previsao na lista predict\n",
    "    prediction.append(p)\n",
    "\n",
    "    # adicionar soma para cálculo do custo\n",
    "    w_sum.append(soma)\n",
    "    \n",
    "# cálculo do custo\n",
    "loss = cross_entropy_loss(w_sum, target1, prediction)\n",
    "\n",
    "certos = 0\n",
    "for y, pred in zip(target1, prediction):\n",
    "    if y == pred:\n",
    "        certos = certos + 1\n",
    "        \n",
    "acuracia = certos / len(target1)\n",
    "\n",
    "\n",
    "media = sum(w_sum)/len(w_sum)\n",
    "\n",
    "print(f\"Média Predictions: {media}\")\n",
    "print(f\"Acurácia: {acuracia}\")\n",
    "print(f\"Avarage Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73030c9",
   "metadata": {},
   "source": [
    "### Comparação usando TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d689bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13328/13328 [==============================] - 27s 2ms/step - loss: 0.5307 - accuracy: 0.8351\n",
      "Epoch 2/100\n",
      "13328/13328 [==============================] - 27s 2ms/step - loss: 0.3922 - accuracy: 0.8908\n",
      "Epoch 3/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.3414 - accuracy: 0.8974\n",
      "Epoch 4/100\n",
      "13328/13328 [==============================] - 27s 2ms/step - loss: 0.3125 - accuracy: 0.9019\n",
      "Epoch 5/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.2934 - accuracy: 0.9060\n",
      "Epoch 6/100\n",
      "13328/13328 [==============================] - 20s 1ms/step - loss: 0.2795 - accuracy: 0.9090\n",
      "Epoch 7/100\n",
      "13328/13328 [==============================] - 19s 1ms/step - loss: 0.2690 - accuracy: 0.9110\n",
      "Epoch 8/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.2607 - accuracy: 0.9123\n",
      "Epoch 9/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.2539 - accuracy: 0.9134\n",
      "Epoch 10/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.2482 - accuracy: 0.9143\n",
      "Epoch 11/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2433 - accuracy: 0.9153\n",
      "Epoch 12/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.2391 - accuracy: 0.9159\n",
      "Epoch 13/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.2355 - accuracy: 0.9165\n",
      "Epoch 14/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.2322 - accuracy: 0.9172\n",
      "Epoch 15/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.2293 - accuracy: 0.9178\n",
      "Epoch 16/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2267 - accuracy: 0.9183\n",
      "Epoch 17/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2243 - accuracy: 0.9187\n",
      "Epoch 18/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2221 - accuracy: 0.9190\n",
      "Epoch 19/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2201 - accuracy: 0.9195\n",
      "Epoch 20/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.2183 - accuracy: 0.9199\n",
      "Epoch 21/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2165 - accuracy: 0.9203\n",
      "Epoch 22/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2149 - accuracy: 0.9206\n",
      "Epoch 23/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.2134 - accuracy: 0.9210\n",
      "Epoch 24/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2120 - accuracy: 0.9213\n",
      "Epoch 25/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.2106 - accuracy: 0.9217\n",
      "Epoch 26/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.2094 - accuracy: 0.9219\n",
      "Epoch 27/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2082 - accuracy: 0.9223\n",
      "Epoch 28/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2070 - accuracy: 0.9226\n",
      "Epoch 29/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.2059 - accuracy: 0.9229\n",
      "Epoch 30/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2049 - accuracy: 0.9231\n",
      "Epoch 31/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.2039 - accuracy: 0.9235\n",
      "Epoch 32/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2029 - accuracy: 0.9238\n",
      "Epoch 33/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2020 - accuracy: 0.9241\n",
      "Epoch 34/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.2011 - accuracy: 0.9243\n",
      "Epoch 35/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.2003 - accuracy: 0.9246\n",
      "Epoch 36/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1995 - accuracy: 0.9249\n",
      "Epoch 37/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1987 - accuracy: 0.9252\n",
      "Epoch 38/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1979 - accuracy: 0.9254\n",
      "Epoch 39/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1972 - accuracy: 0.9256\n",
      "Epoch 40/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1965 - accuracy: 0.9259\n",
      "Epoch 41/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1958 - accuracy: 0.9263\n",
      "Epoch 42/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1951 - accuracy: 0.9266\n",
      "Epoch 43/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1945 - accuracy: 0.9268\n",
      "Epoch 44/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1938 - accuracy: 0.9271\n",
      "Epoch 45/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1932 - accuracy: 0.9273\n",
      "Epoch 46/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1926 - accuracy: 0.9276\n",
      "Epoch 47/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1921 - accuracy: 0.9279\n",
      "Epoch 48/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1915 - accuracy: 0.9281\n",
      "Epoch 49/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1910 - accuracy: 0.9284\n",
      "Epoch 50/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1904 - accuracy: 0.9285\n",
      "Epoch 51/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1899 - accuracy: 0.9288\n",
      "Epoch 52/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1894 - accuracy: 0.9290\n",
      "Epoch 53/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1889 - accuracy: 0.9292\n",
      "Epoch 54/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1884 - accuracy: 0.9295\n",
      "Epoch 55/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1879 - accuracy: 0.9297\n",
      "Epoch 56/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1875 - accuracy: 0.9299\n",
      "Epoch 57/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1870 - accuracy: 0.9301\n",
      "Epoch 58/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1866 - accuracy: 0.9303\n",
      "Epoch 59/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1861 - accuracy: 0.9305\n",
      "Epoch 60/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.1857 - accuracy: 0.9306\n",
      "Epoch 61/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1853 - accuracy: 0.9309\n",
      "Epoch 62/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1849 - accuracy: 0.9310\n",
      "Epoch 63/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1845 - accuracy: 0.9312\n",
      "Epoch 64/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1841 - accuracy: 0.9314\n",
      "Epoch 65/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1837 - accuracy: 0.9315\n",
      "Epoch 66/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1834 - accuracy: 0.9318\n",
      "Epoch 67/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.1830 - accuracy: 0.9320\n",
      "Epoch 68/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.1826 - accuracy: 0.9321\n",
      "Epoch 69/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1823 - accuracy: 0.9323\n",
      "Epoch 70/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1820 - accuracy: 0.9324\n",
      "Epoch 71/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1816 - accuracy: 0.9326\n",
      "Epoch 72/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1813 - accuracy: 0.9328\n",
      "Epoch 73/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1810 - accuracy: 0.9329\n",
      "Epoch 74/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1806 - accuracy: 0.9331\n",
      "Epoch 75/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1803 - accuracy: 0.9332\n",
      "Epoch 76/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.1800 - accuracy: 0.9334\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1797 - accuracy: 0.9335\n",
      "Epoch 78/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1794 - accuracy: 0.9337\n",
      "Epoch 79/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1791 - accuracy: 0.9337\n",
      "Epoch 80/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1788 - accuracy: 0.9339\n",
      "Epoch 81/100\n",
      "13328/13328 [==============================] - 27s 2ms/step - loss: 0.1786 - accuracy: 0.9341\n",
      "Epoch 82/100\n",
      "13328/13328 [==============================] - 27s 2ms/step - loss: 0.1783 - accuracy: 0.9342\n",
      "Epoch 83/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1780 - accuracy: 0.9344\n",
      "Epoch 84/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1778 - accuracy: 0.9345\n",
      "Epoch 85/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1775 - accuracy: 0.9346\n",
      "Epoch 86/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1772 - accuracy: 0.9347\n",
      "Epoch 87/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1770 - accuracy: 0.9349\n",
      "Epoch 88/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1767 - accuracy: 0.9350\n",
      "Epoch 89/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1765 - accuracy: 0.9351\n",
      "Epoch 90/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1762 - accuracy: 0.9352\n",
      "Epoch 91/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1760 - accuracy: 0.9354\n",
      "Epoch 92/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1758 - accuracy: 0.9354\n",
      "Epoch 93/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1756 - accuracy: 0.9356\n",
      "Epoch 94/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1753 - accuracy: 0.9357\n",
      "Epoch 95/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.1751 - accuracy: 0.9358\n",
      "Epoch 96/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1749 - accuracy: 0.9359\n",
      "Epoch 97/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1747 - accuracy: 0.9360\n",
      "Epoch 98/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.1745 - accuracy: 0.9361\n",
      "Epoch 99/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1742 - accuracy: 0.9362\n",
      "Epoch 100/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.1740 - accuracy: 0.9363\n",
      "4418/4418 [==============================] - 4s 928us/step\n",
      "Accuracy: 0.500581585997753\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A baixa acurácia no teste pode indicar um overfitting do modelo criado\n",
    "'''\n",
    "\n",
    "# baseado na aula sugerida do MIT\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "X_train = treino.drop(\"Class\", axis=1)\n",
    "y_train = treino[\"Class\"]\n",
    "X_test = teste.drop(\"Class\", axis=1)\n",
    "y_test = teste[\"Class\"]\n",
    "\n",
    "# numero de features\n",
    "num_features = len(credit_norm.columns) - 1\n",
    "\n",
    "# criação do modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# compilar modelo\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# treinar modelo\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# predict\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# converter predições para binário\n",
    "threshold = 0.5\n",
    "binary_predictions = (predictions > threshold).astype(int)\n",
    "\n",
    "# printar acurácia do modelo\n",
    "certos = 0\n",
    "for y, pred in zip(y_test, binary_predictions):\n",
    "    if y == pred[0]:\n",
    "        certos = certos + 1\n",
    "        \n",
    "acuracia = certos / len(target1)\n",
    "\n",
    "\n",
    "media = sum(w_sum)/len(w_sum)\n",
    "\n",
    "print(f\"Accuracy: {media}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317b0df",
   "metadata": {},
   "source": [
    "### Modelo do TensorFlow usando regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba30dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6240 - accuracy: 0.8068\n",
      "Epoch 2/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6056 - accuracy: 0.8567\n",
      "Epoch 3/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6049 - accuracy: 0.8594\n",
      "Epoch 4/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6047 - accuracy: 0.8601\n",
      "Epoch 5/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6047 - accuracy: 0.8607\n",
      "Epoch 6/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8610\n",
      "Epoch 7/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8612\n",
      "Epoch 8/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6047 - accuracy: 0.8612\n",
      "Epoch 9/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 10/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 11/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6047 - accuracy: 0.8616\n",
      "Epoch 12/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 13/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6047 - accuracy: 0.8617\n",
      "Epoch 14/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 15/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6047 - accuracy: 0.8616\n",
      "Epoch 16/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 17/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 18/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6047 - accuracy: 0.8615\n",
      "Epoch 19/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 20/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8609\n",
      "Epoch 21/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 22/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 23/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8619\n",
      "Epoch 24/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8612\n",
      "Epoch 25/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 26/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6047 - accuracy: 0.8615\n",
      "Epoch 27/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 28/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 29/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8613\n",
      "Epoch 30/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 31/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8610\n",
      "Epoch 32/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 33/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 34/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 35/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 36/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 37/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 38/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6047 - accuracy: 0.8615\n",
      "Epoch 39/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 40/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 41/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 42/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 43/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6047 - accuracy: 0.8616\n",
      "Epoch 44/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 45/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 46/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 47/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 48/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 49/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 50/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 51/100\n",
      "13328/13328 [==============================] - 27s 2ms/step - loss: 0.6046 - accuracy: 0.8613\n",
      "Epoch 52/100\n",
      "13328/13328 [==============================] - 34s 3ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 53/100\n",
      "13328/13328 [==============================] - 20s 1ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 54/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 55/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 56/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 57/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 58/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8611\n",
      "Epoch 59/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 60/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 61/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 62/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8613\n",
      "Epoch 63/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 64/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8613\n",
      "Epoch 65/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 66/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 67/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 68/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 69/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8612\n",
      "Epoch 70/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 71/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 72/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 73/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 74/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6047 - accuracy: 0.8617\n",
      "Epoch 75/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 76/100\n",
      "13328/13328 [==============================] - 21s 2ms/step - loss: 0.6046 - accuracy: 0.8611\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 78/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 79/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6046 - accuracy: 0.8613\n",
      "Epoch 80/100\n",
      "13328/13328 [==============================] - 29s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 81/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.6046 - accuracy: 0.8620\n",
      "Epoch 82/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 83/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 84/100\n",
      "13328/13328 [==============================] - 26s 2ms/step - loss: 0.6046 - accuracy: 0.8617\n",
      "Epoch 85/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 86/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 87/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6046 - accuracy: 0.8611\n",
      "Epoch 88/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8619\n",
      "Epoch 89/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6046 - accuracy: 0.8612\n",
      "Epoch 90/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 91/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8618\n",
      "Epoch 92/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 93/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 94/100\n",
      "13328/13328 [==============================] - 25s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 95/100\n",
      "13328/13328 [==============================] - 24s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 96/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 97/100\n",
      "13328/13328 [==============================] - 22s 2ms/step - loss: 0.6046 - accuracy: 0.8616\n",
      "Epoch 98/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8615\n",
      "Epoch 99/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6046 - accuracy: 0.8614\n",
      "Epoch 100/100\n",
      "13328/13328 [==============================] - 23s 2ms/step - loss: 0.6047 - accuracy: 0.8616\n",
      "4418/4418 [==============================] - 6s 1ms/step\n",
      "Accuracy: 0.500581585997753\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Aplicação de regularização L2 para tentar melhorar a generalização do modelo\n",
    "'''\n",
    "\n",
    "X_train = treino.drop(\"Class\", axis=1)\n",
    "y_train = treino[\"Class\"]\n",
    "X_test = teste.drop(\"Class\", axis=1)\n",
    "y_test = teste[\"Class\"]\n",
    "\n",
    "# numero de features\n",
    "num_features = len(credit_norm.columns) - 1\n",
    "\n",
    "# criação do modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "# compilar modelo\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# treinar modelo\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# predict\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# converter predições para binário\n",
    "threshold = 0.5\n",
    "binary_predictions = (predictions > threshold).astype(int)\n",
    "\n",
    "# printar acurácia do modelo\n",
    "certos = 0\n",
    "for y, pred in zip(y_test, binary_predictions):\n",
    "    if y == pred[0]:\n",
    "        certos = certos + 1\n",
    "        \n",
    "acuracia = certos / len(target1)\n",
    "\n",
    "\n",
    "media = sum(w_sum)/len(w_sum)\n",
    "\n",
    "print(f\"Accuracy: {media}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
